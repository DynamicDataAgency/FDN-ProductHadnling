{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query FTP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### jupyter nbconvert --to script Continue-Update-Process.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*cmd* 'USER ps-ftp_5567077'\n",
      "*put* 'USER ps-ftp_5567077\\r\\n'\n",
      "*get* '331 User name okay, need password for ps-ftp_5567077.\\n'\n",
      "*resp* '331 User name okay, need password for ps-ftp_5567077.'\n",
      "*cmd* 'PASS **********'\n",
      "*put* 'PASS **********\\r\\n'\n",
      "*get* '230 User logged in, proceed.\\n'\n",
      "*resp* '230 User logged in, proceed.'\n",
      "*cmd* 'CWD /Fanatics-(Global)/'\n",
      "*put* 'CWD /Fanatics-(Global)/\\r\\n'\n",
      "*get* '250 Directory changed to /Fanatics-(Global)/\\n'\n",
      "*resp* '250 Directory changed to /Fanatics-(Global)/'\n",
      "*cmd* 'TYPE A'\n",
      "*put* 'TYPE A\\r\\n'\n",
      "*get* '200 Command TYPE okay.\\n'\n",
      "*resp* '200 Command TYPE okay.'\n",
      "*cmd* 'PASV'\n",
      "*put* 'PASV\\r\\n'\n",
      "*get* '227 Entering Passive Mode (35,207,5,176,169,191)\\n'\n",
      "*resp* '227 Entering Passive Mode (35,207,5,176,169,191)'\n",
      "*cmd* 'NLST'\n",
      "*put* 'NLST\\r\\n'\n",
      "*get* '150 File status okay; about to open data connection.\\n'\n",
      "*resp* '150 File status okay; about to open data connection.'\n",
      "*get* '226 Closing data connection.\\n'\n",
      "*resp* '226 Closing data connection.'\n",
      "Files in the directory:\n",
      "Fanatics-Product-Catalog_CUSTOM.txt.gz\n",
      "Fanatics-Product-Catalog_IR.txt.gz\n",
      "Fanatics-Top-Products_CUSTOM.txt.gz\n",
      "Fanatics-Top-Products_IR.txt.gz\n",
      "Fanatics-UK_CUSTOM.txt.gz\n",
      "Fanatics-UK_IR.txt.gz\n",
      "Fanatics-France_CUSTOM.txt.gz\n",
      "Fanatics-France_IR.txt.gz\n",
      "Fanatics-Spain_CUSTOM.txt.gz\n",
      "Fanatics-Spain_IR.txt.gz\n",
      "Fanatics-Germany_CUSTOM.txt.gz\n",
      "Fanatics-Germany_IR.txt.gz\n",
      "Fanatics-Australia_CUSTOM.txt.gz\n",
      "Fanatics-Australia_IR.txt.gz\n",
      "Fanatics-Italy_CUSTOM.txt.gz\n",
      "Fanatics-Italy_IR.txt.gz\n",
      "*cmd* 'TYPE I'\n",
      "*put* 'TYPE I\\r\\n'\n",
      "*get* '200 Command TYPE okay.\\n'\n",
      "*resp* '200 Command TYPE okay.'\n",
      "*cmd* 'PASV'\n",
      "*put* 'PASV\\r\\n'\n",
      "*get* '227 Entering Passive Mode (35,207,5,176,216,24)\\n'\n",
      "*resp* '227 Entering Passive Mode (35,207,5,176,216,24)'\n",
      "*cmd* 'RETR Fanatics-Product-Catalog_IR.txt.gz'\n",
      "*put* 'RETR Fanatics-Product-Catalog_IR.txt.gz\\r\\n'\n",
      "*get* '150 File status okay; about to open data connection.\\n'\n",
      "*resp* '150 File status okay; about to open data connection.'\n",
      "*get* '226 Transfer complete.\\n'\n",
      "*resp* '226 Transfer complete.'\n",
      "Downloaded Fanatics-Product-Catalog_IR.txt.gz to data-update-process\n",
      "*cmd* 'QUIT'\n",
      "*put* 'QUIT\\r\\n'\n",
      "*get* '221 Goodbye.\\n'\n",
      "*resp* '221 Goodbye.'\n"
     ]
    }
   ],
   "source": [
    "import ftplib\n",
    "import os\n",
    "import time\n",
    "\n",
    "# FTP server details\n",
    "ftp_server = \"products.impact.com\"\n",
    "ftp_user = \"ps-ftp_5567077\"\n",
    "ftp_password = \"6r%]mobnH6\"\n",
    "ftp_directory = \"/Fanatics-(Global)/\"\n",
    "file_to_download = \"Fanatics-Product-Catalog_IR.txt.gz\"  # Specify the file to download\n",
    "\n",
    "# Local directory to save the file\n",
    "local_directory = \"data-update-process\"\n",
    "os.makedirs(local_directory, exist_ok=True)  # Create the directory if it doesn't exist\n",
    "\n",
    "max_retries = 3\n",
    "retry_delay = 5  # Delay in seconds between retries\n",
    "\n",
    "def download_file():\n",
    "    \"\"\"Attempt to connect to FTP and download the specified file.\"\"\"\n",
    "    try:\n",
    "        # Connect to the FTP server\n",
    "        ftp = ftplib.FTP(ftp_server)\n",
    "        ftp.set_debuglevel(2)  # Enable FTP command logging\n",
    "        ftp.set_pasv(True)  # Enable passive mode\n",
    "\n",
    "        ftp.login(user=ftp_user, passwd=ftp_password)\n",
    "        ftp.cwd(ftp_directory)\n",
    "\n",
    "        # List files in the directory\n",
    "        files = ftp.nlst()\n",
    "        print(\"Files in the directory:\")\n",
    "        for file in files:\n",
    "            print(file)\n",
    "\n",
    "        # Download only the specified file\n",
    "        if file_to_download in files:\n",
    "            local_filename = os.path.join(local_directory, file_to_download)  # Save in the folder\n",
    "            with open(local_filename, 'wb') as local_file:\n",
    "                ftp.retrbinary('RETR ' + file_to_download, local_file.write)\n",
    "                print(f\"Downloaded {file_to_download} to {local_directory}\")\n",
    "        else:\n",
    "            print(f\"{file_to_download} not found in the directory.\")\n",
    "        \n",
    "        ftp.quit()  # Ensure connection is closed\n",
    "\n",
    "    except ftplib.all_errors as e:\n",
    "        print(f\"FTP error: {e}\")\n",
    "        raise  # Raise the error to be handled by the retry mechanism\n",
    "\n",
    "\n",
    "# Retry mechanism\n",
    "for attempt in range(max_retries):\n",
    "    try:\n",
    "        download_file()  # Call the function to perform the FTP download\n",
    "        break  # If the download succeeds, exit the retry loop\n",
    "    except (ftplib.error_temp, ConnectionResetError) as e:\n",
    "        print(f\"Temporary error occurred: {e}. Retrying ({attempt + 1}/{max_retries})...\")\n",
    "        if attempt == max_retries - 1:\n",
    "            print(\"Max retries reached. Exiting.\")\n",
    "            raise e  # Re-raise the exception if all retries fail\n",
    "        time.sleep(retry_delay)  # Wait before retrying\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompress downloaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Compressed file ended before the end-of-stream marker was reached",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(local_gz_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(local_txt_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out: \n\u001b[1;32m---> 17\u001b[0m         \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecompressed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_gz_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocal_txt_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\Lib\\shutil.py:203\u001b[0m, in \u001b[0;36mcopyfileobj\u001b[1;34m(fsrc, fdst, length)\u001b[0m\n\u001b[0;32m    201\u001b[0m fsrc_read \u001b[38;5;241m=\u001b[39m fsrc\u001b[38;5;241m.\u001b[39mread\n\u001b[0;32m    202\u001b[0m fdst_write \u001b[38;5;241m=\u001b[39m fdst\u001b[38;5;241m.\u001b[39mwrite\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m buf \u001b[38;5;241m:=\u001b[39m \u001b[43mfsrc_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    204\u001b[0m     fdst_write(buf)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\Lib\\gzip.py:324\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\Lib\\_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[1;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.1776.0_x64__qbz5n2kfra8p0\\Lib\\gzip.py:547\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 547\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompressed file ended before the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    548\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend-of-stream marker was reached\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc \u001b[38;5;241m=\u001b[39m zlib\u001b[38;5;241m.\u001b[39mcrc32(uncompress, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crc)\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(uncompress)\n",
      "\u001b[1;31mEOFError\u001b[0m: Compressed file ended before the end-of-stream marker was reached"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Directory containing the downloaded .txt.gz files\n",
    "directory = os.path.join(os.getcwd(), \"data-update-process\")  # Path to the downloaded files\n",
    "\n",
    "# Decompress each .txt.gz file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt.gz'):\n",
    "        local_gz_filename = os.path.join(directory, filename)\n",
    "        local_txt_filename = os.path.join(directory, filename[:-3])  # Remove the .gz extension\n",
    "\n",
    "        # Decompress the .txt.gz file to .txt\n",
    "        with gzip.open(local_gz_filename, 'rb') as f_in:\n",
    "            with open(local_txt_filename, 'wb') as f_out: \n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "                print(f\"Decompressed {local_gz_filename} to {local_txt_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unique Merchant SKU                                       Product Name  \\\n",
      "0              3180765  Men's New Era Black Minnesota Vikings Black On...   \n",
      "1              5257010  Men's Under Armour Gray Virginia Tech Hokies S...   \n",
      "2            200006345  Women's '47  Navy Las Vegas Raiders Primrose C...   \n",
      "3            200180351  Women's Antigua  Navy Jacksonville Jumbo Shrim...   \n",
      "4            200465364   Keyscaper  Rey Mysterio Galaxy Impact Clear Case   \n",
      "\n",
      "                                         Product URL  \\\n",
      "0  https://fanatics.93n6tx.net/c/5567077/669352/9...   \n",
      "1  https://fanatics.93n6tx.net/c/5567077/669352/9...   \n",
      "2  https://fanatics.93n6tx.net/c/5567077/669352/9...   \n",
      "3  https://fanatics.93n6tx.net/c/5567077/669352/9...   \n",
      "4  https://fanatics.93n6tx.net/c/5567077/669352/9...   \n",
      "\n",
      "                                           Image URL  Current Price  \\\n",
      "0  https://feeds.frgimages.com/_pi3180000_ff_3180...          26.24   \n",
      "1  https://feeds.frgimages.com/_pi5257000_ff_5257...          39.99   \n",
      "2  https://feeds.frgimages.com/_ss5_p-200006345+u...          23.99   \n",
      "3  https://feeds.frgimages.com/_ss5_p-200180351+u...          52.49   \n",
      "4  https://feeds.frgimages.com/_ss5_p-200465364+u...          26.24   \n",
      "\n",
      "  Stock Availability Condition  EAN  \\\n",
      "0                  Y       New  NaN   \n",
      "1                  Y       New  NaN   \n",
      "2                  Y       New  NaN   \n",
      "3                  Y       New  NaN   \n",
      "4                  Y       New  NaN   \n",
      "\n",
      "                                                 UPC ISBN  ...  Numeric1  \\\n",
      "0                                       192859235471  NaN  ...       NaN   \n",
      "1  197141891427;197141891397;197141891441;1971418...  NaN  ...       NaN   \n",
      "2                                       196505886789  NaN  ...       NaN   \n",
      "3  197719364117;197719364094;197719364100;1977193...  NaN  ...       NaN   \n",
      "4  197281171571;197281171847;197281171755;1972811...  NaN  ...       NaN   \n",
      "\n",
      "   Numeric2  Numeric3  Money1  Money2 Money3 Currency  Shipping Region Labels  \\\n",
      "0      0.41       NaN     NaN     NaN    NaN      USD              NaN    NaN   \n",
      "1      0.41       NaN     NaN     NaN    NaN      USD              NaN    NaN   \n",
      "2      0.35       NaN     NaN     NaN    NaN      USD              NaN    NaN   \n",
      "3      0.41       NaN     NaN     NaN    NaN      USD              NaN    NaN   \n",
      "4      0.50       NaN     NaN     NaN    NaN      USD              NaN    NaN   \n",
      "\n",
      "   Promotion  \n",
      "0        NaN  \n",
      "1        NaN  \n",
      "2        NaN  \n",
      "3        NaN  \n",
      "4        NaN  \n",
      "\n",
      "[5 rows x 70 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "# Get the current working directory\n",
    "directory = os.path.join(os.getcwd(), \"data-update-process\")\n",
    "\n",
    "# Path to the decompressed .txt file\n",
    "local_txt_filename = os.path.join(directory, \"Fanatics-Product-Catalog_IR.txt\")\n",
    "\n",
    "# Load the decompressed .txt file into a Pandas DataFrame\n",
    "if os.path.exists(local_txt_filename):\n",
    "    # Assuming the file is tab-delimited, adjust the delimiter if necessary\n",
    "    df = pd.read_csv(local_txt_filename, delimiter='\\t', low_memory=False)\n",
    "    print(df.head())  # Display the first 5 rows\n",
    "else:\n",
    "    print(f\"{local_txt_filename} does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter needed products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[df[\"Category\"] == \"NFL\"]\n",
    "df=df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 70)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations to fit Shopify product import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Count the occurrences of each value in the 'Text3' column of the 'nf' dataframe\n",
    "# text3_counts = nf['Text3'].value_counts()\n",
    "\n",
    "# Print the top 60 values\n",
    "#  print(text3_counts.head(60))\n",
    "\n",
    "nf = df.loc[:, ['Unique Merchant SKU', 'Product Name', 'Product URL', 'Image URL', 'Current Price', 'Original Price', 'Product Description', 'Gender', 'Text1', 'Text2', 'Text3','Size']]\n",
    "\n",
    "rename_dict = {\n",
    "    'Unique Merchant SKU': 'ID',\n",
    "    'Product Name': 'Title',\n",
    "    'Product URL': 'Metafield: custom.product_url [url]',\n",
    "    'Image URL': 'Image Src',\n",
    "    'Current Price': 'Variant Price',\n",
    "    'Original Price': 'Variant Compare At Price',\n",
    "    'Product Description': 'Body HTML',\n",
    "    'Gender': 'Metafield: custom.product_gender [single_line_text_field]',\n",
    "    'Text1': 'Metafield: custom.text1 [single_line_text_field]',\n",
    "    'Text2': 'Metafield: custom.text2 [single_line_text_field]',\n",
    "    'Text3': 'Tags',\n",
    "    'Size': 'Metafield: custom.sizes [single_line_text_field]'\n",
    "}\n",
    "\n",
    "nf = nf.rename(columns=rename_dict)\n",
    "\n",
    "\n",
    "\n",
    "## remove emptyspaces in Tags column between values\n",
    "nf['Tags'] = nf['Tags'].str.replace(' ', '')\n",
    "\n",
    "# Create an additional column 'Variant Inventory Tracker' with all values value 'shopify'\n",
    "nf['Variant Inventory Tracker'] = 'shopify'\n",
    "nf['Variant Inventory Policy'] = 'continue'\n",
    "nf['Variant Fulfillment Service'] = 'manual'\n",
    "nf['Variant Inventory Qty'] = '1'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to google sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import set_with_dataframe\n",
    "import os\n",
    "import dotenv\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define the scope\n",
    "scope = [\"https://spreadsheets.google.com/feeds\", 'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "\n",
    "\n",
    "# Authorize using the JSON file directly\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name('credentials.json', scope)\n",
    "\n",
    "# Authorize the clientsheet\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "# Open the Google Sheet (by name or by key)\n",
    "spreadsheet = client.open(\"Fanatics_product_import\")\n",
    "\n",
    "# Select the first sheet (or specify another sheet)\n",
    "sheet = spreadsheet.sheet1  \n",
    "\n",
    "# Clear existing data in the sheet (optional)\n",
    "sheet.clear()\n",
    "\n",
    "# Write DataFrame to Google Sheet (ensure 'nf' is a valid DataFrame)\n",
    "set_with_dataframe(sheet, nf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
